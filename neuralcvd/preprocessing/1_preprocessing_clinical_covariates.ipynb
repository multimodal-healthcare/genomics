{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-04T12:31:49.436340Z",
     "start_time": "2020-11-04T12:31:48.732042Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import yaml\n",
    "from tqdm.notebook import trange, tqdm\n",
    "dataset_name = \"210212_cvd_gp\"\n",
    "path = \"/data/analysis/ag-reils/steinfej/code/umbrella/pre/ukbb\"\n",
    "data_path = \"/data/analysis/ag-reils/ag-reils-shared/cardioRS/data\"\n",
    "dataset_path = f\"{data_path}/2_datasets_pre/{dataset_name}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-04T12:31:49.895222Z",
     "start_time": "2020-11-04T12:31:49.891332Z"
    }
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "Path(dataset_path).mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-04T12:33:14.171198Z",
     "start_time": "2020-11-04T12:31:50.204540Z"
    }
   },
   "outputs": [],
   "source": [
    "data = pd.read_feather(f\"{data_path}/1_decoded/ukb_data.feather\")\n",
    "data_field = pd.read_feather(f\"{data_path}/1_decoded/ukb_data_field.feather\")\n",
    "data_columns = data.columns.to_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mappings + Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-04T12:34:05.867152Z",
     "start_time": "2020-11-04T12:33:16.878773Z"
    }
   },
   "outputs": [],
   "source": [
    "# Drop obviouse missing data\n",
    "print(len(data))\n",
    "data = data.dropna(subset=[\"sex_f31_0_0\"], axis=0)\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Starting information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-04T12:34:05.872216Z",
     "start_time": "2020-11-04T12:34:05.869505Z"
    }
   },
   "outputs": [],
   "source": [
    "#time0_col=\"birth_date\"\n",
    "time0_col=\"date_of_attending_assessment_centre_f53_0_0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_field"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline covariates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-04T12:34:05.889725Z",
     "start_time": "2020-11-04T12:34:05.874587Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_fields(fields, data, data_field):\n",
    "    f = data_field[data_field[\"field.showcase\"].isin(fields) & data_field[\"field.tab\"].str.contains(\"f\\\\.\\\\d+\\\\.0\\\\.\\\\d\")].copy()\n",
    "    f[\"field\"] = pd.Categorical(f[\"field.showcase\"], categories=fields, ordered=True)\n",
    "    f = f.sort_values(\"field\").reset_index().drop(\"field\", axis=1)\n",
    "    return f\n",
    "\n",
    "def get_fields_all(fields, data, data_field):\n",
    "    f = data_field[data_field[\"field.showcase\"].isin(fields)].copy()\n",
    "    f[\"field\"] = pd.Categorical(f[\"field.showcase\"], categories=fields, ordered=True)\n",
    "    f = f.sort_values(\"field\").reset_index().drop(\"field\", axis=1)\n",
    "    return f\n",
    "\n",
    "def get_data_fields(fields, data, data_field):\n",
    "    f = get_fields(fields, data, data_field)\n",
    "    return data[[\"eid\"]+f[\"col.name\"].to_list()].copy()\n",
    "\n",
    "def get_data_fields_all(fields, data, data_field):\n",
    "    f = get_fields_all(fields, data, data_field)\n",
    "    return data[[\"eid\"]+f[\"col.name\"].to_list()].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coding10 = pd.read_csv(f\"{data_path}/mapping/codings/coding10.tsv\", sep=\"\\t\").assign(coding = lambda x: x.coding.astype(\"int\")).rename(columns={\"coding\":\"uk_biobank_assessment_centre_f54_0_0\"})\n",
    "coding10[\"uk_biobank_assessment_centre_f54_0_0\"] = coding10[\"uk_biobank_assessment_centre_f54_0_0\"].astype(\"int\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-04T12:34:06.176411Z",
     "start_time": "2020-11-04T12:34:05.891730Z"
    }
   },
   "outputs": [],
   "source": [
    "fields_basics = [\n",
    "    \"21022\", # age at recruitment\n",
    "    \"31\", # sex\n",
    "    \"21000\", # ethnicity\n",
    "    \"189\", # Townsend index\n",
    "    \"53\", # date of baseline assessment\n",
    "    \"54\", # assessment center\n",
    "]\n",
    "\n",
    "temp = get_data_fields(fields_basics, data, data_field)\n",
    "\n",
    "temp[\"sex_f31_0_0\"] = temp[\"sex_f31_0_0\"].cat.set_categories([\"Female\", 'Male'], ordered=False)\n",
    "\n",
    "#temp[\"ethnic_background_f21000_0_0\"] = temp[\"ethnic_background_f21000_0_0\"].astype(\"string\")\n",
    "\n",
    "ethn_bg_def = {\"White\": [\"White\", \"British\", \"Irish\", \"Any other white background\"],\n",
    "                \"Mixed\": [\"Mixed\", \"White and Black Caribbean\", \"White and Black African\", \"White and Asian\", \"Any other mixed background\"],  \n",
    "                \"Asian\": [\"Asian or Asian British\", \"Indian\", \"Pakistani\", \"Bangladeshi\", \"Any other Asian background\"], \n",
    "                \"Black\": [\"Black or Black British\", \"Caribbean\", \"African\", \"Any other Black background\"],\n",
    "                \"Chinese\": [\"Chinese\"],  \n",
    "                np.nan: [\"Other ethnic group\", \"Do not know\", \"Prefer not to answer\"]}\n",
    "\n",
    "ethn_bg_dict = {}\n",
    "for key, values in ethn_bg_def.items(): \n",
    "    for value in values:\n",
    "        ethn_bg_dict[value]=key \n",
    "        \n",
    "temp[\"ethnic_background_f21000_0_0\"].replace(ethn_bg_dict, inplace=True)\n",
    "temp[\"ethnic_background_f21000_0_0\"] = temp[\"ethnic_background_f21000_0_0\"].astype(\"category\")\n",
    "\n",
    "#\n",
    "#temp[\"ethnic_background_f21000_0_0\"] = temp[\"ethnic_background_f21000_0_0\"].astype(\"category\").cat.set_categories(['White', 'Black', 'Asien', 'Mixed', 'Chinese'], ordered=False)\n",
    "\n",
    "basics = temp\n",
    "print(len(temp))\n",
    "\n",
    "from dateutil.relativedelta import relativedelta\n",
    "calc_birth_date = [date_of_attending_assessment_centre - relativedelta(years=age_at_recruitment) \n",
    "                                                             for date_of_attending_assessment_centre, age_at_recruitment \n",
    "                                                             in zip(basics[\"date_of_attending_assessment_centre_f53_0_0\"], basics[\"age_at_recruitment_f21022_0_0\"])]\n",
    "\n",
    "basics = basics.assign(birth_date = calc_birth_date)\n",
    "basics[\"uk_biobank_assessment_centre_f54_0_0\"] = basics.assign(uk_biobank_assessment_centre_f54_0_0 = lambda x: x.uk_biobank_assessment_centre_f54_0_0.astype(\"int\")).merge(coding10, on=\"uk_biobank_assessment_centre_f54_0_0\")[\"meaning\"]\n",
    "\n",
    "\n",
    "display(basics.head())\n",
    "basics.to_feather(os.path.join(path, dataset_path, 'temp_basics.feather'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-04T12:34:06.186613Z",
     "start_time": "2020-11-04T12:34:06.178111Z"
    }
   },
   "outputs": [],
   "source": [
    " print(temp[\"ethnic_background_f21000_0_0\"].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questionnaire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-04T12:34:06.389467Z",
     "start_time": "2020-11-04T12:34:06.188206Z"
    }
   },
   "outputs": [],
   "source": [
    "fields_questionnaire = [\n",
    "    \"2178\", # Overall health\n",
    "    \"20116\", # Smoking status\n",
    "    \"1558\",\n",
    "]\n",
    "\n",
    "temp = get_data_fields(fields_questionnaire, data, data_field)\n",
    "\n",
    "temp[\"overall_health_rating_f2178_0_0\"] = temp[\"overall_health_rating_f2178_0_0\"]\\\n",
    "    .replace({\"Do not know\": np.nan, \"Prefer not to answer\": np.nan})\\\n",
    "    .astype(\"category\").cat.set_categories(['Poor', 'Fair', 'Good', 'Excellent'], ordered=True)\n",
    "\n",
    "\n",
    "temp[\"smoking_status_f20116_0_0\"] = temp[\"smoking_status_f20116_0_0\"]\\\n",
    "    .replace({\"Prefer not to answer\": np.nan}, inplace=False)\\\n",
    "    .astype(\"category\").cat.set_categories(['Current', 'Previous', 'Never'], ordered=True)\n",
    "\n",
    "temp[\"alcohol_intake_frequency_f1558_0_0\"] = temp[\"alcohol_intake_frequency_f1558_0_0\"]\\\n",
    "    .replace({\"Prefer not to answer\": np.nan}, inplace=False)\\\n",
    "    .astype(\"category\").cat.set_categories([\n",
    "        'Daily or almost daily', \n",
    "        'Three or four times a week', \n",
    "        'Once or twice a week',\n",
    "        'One to three times a month',\n",
    "        'Special occasions only', \n",
    "        'Never'], ordered=True)\n",
    "\n",
    "questionnaire = temp\n",
    "print(len(temp))\n",
    "display(temp.head())\n",
    "\n",
    "questionnaire.to_feather(os.path.join(path, dataset_path, 'temp_questionnaire.feather'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-04T12:34:06.398168Z",
     "start_time": "2020-11-04T12:34:06.391082Z"
    }
   },
   "outputs": [],
   "source": [
    "print(temp[\"alcohol_intake_frequency_f1558_0_0\"].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Physical measurements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-04T12:34:07.052989Z",
     "start_time": "2020-11-04T12:34:06.400858Z"
    }
   },
   "outputs": [],
   "source": [
    "from statistics import mean\n",
    "\n",
    "fields_measurements = [\n",
    "#    \"100313\", # Walking speed !!! MISSING !!!\n",
    "    \"21001\", # BMI\n",
    "    \"21002\", # weight\n",
    "    \"4080\", # Syst. BP\n",
    "    \"4079\", # Diast. BP\n",
    "    \"102\",\n",
    "    \"21021\",\n",
    "    \"4195\",\n",
    "    \"48\",\n",
    "    \"49\",\n",
    "    \"50\",\n",
    "    \"23127\",\n",
    "    \"23099\",\n",
    "    \"23105\",\n",
    "    \"20151\",\n",
    "    \"20150\",\n",
    "    \"20258\",\n",
    "    \"3064\",\n",
    "    \n",
    "]\n",
    "temp = get_data_fields(fields_measurements, data, data_field)\n",
    "\n",
    "sbp_cols = [\"systolic_blood_pressure_automated_reading_f4080_0_0\", \"systolic_blood_pressure_automated_reading_f4080_0_1\"]\n",
    "dbp_cols = [\"diastolic_blood_pressure_automated_reading_f4079_0_0\", \"diastolic_blood_pressure_automated_reading_f4079_0_1\"]\n",
    "pr_cols = [\"pulse_rate_automated_reading_f102_0_0\", \"pulse_rate_automated_reading_f102_0_1\"]\n",
    "\n",
    "temp = temp.assign(systolic_blood_pressure_automated_reading_f4080 = temp[sbp_cols].mean(axis=1),\n",
    "                   diastolic_blood_pressure_automated_reading_f4079 = temp[dbp_cols].mean(axis=1),\n",
    "                   pulse_rate_automated_reading_f102 = temp[pr_cols].mean(axis=1))\\\n",
    "    .drop(sbp_cols + dbp_cols + pr_cols, axis=1)\n",
    "\n",
    "measurements = temp\n",
    "print(len(temp))\n",
    "display(temp.head())\n",
    "\n",
    "measurements.to_feather(os.path.join(path, dataset_path, 'temp_measurements.feather'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lab measurements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-04T12:34:07.647955Z",
     "start_time": "2020-11-04T12:34:07.055242Z"
    }
   },
   "outputs": [],
   "source": [
    "fields_blood_count = [\n",
    "    \"30160\", #\tBasophill count\n",
    "    \"30220\", #\tBasophill percentage\n",
    "    \"30150\", #\tEosinophill count\n",
    "    \"30210\", #\tEosinophill percentage\n",
    "    \"30030\", #\tHaematocrit percentage\n",
    "    \"30020\", #\tHaemoglobin concentration\n",
    "    \"30300\", #\tHigh light scatter reticulocyte count\n",
    "    \"30290\", #\tHigh light scatter reticulocyte percentage\n",
    "    \"30280\", #\tImmature reticulocyte fraction\n",
    "    \"30120\", #\tLymphocyte count\n",
    "    \"30180\", #\tLymphocyte percentage\n",
    "    \"30050\", #\tMean corpuscular haemoglobin\n",
    "    \"30060\", #\tMean corpuscular haemoglobin concentration\n",
    "    \"30040\", #\tMean corpuscular volume\n",
    "    \"30100\", #\tMean platelet (thrombocyte) volume\n",
    "    \"30260\", #\tMean reticulocyte volume\n",
    "    \"30270\", #\tMean sphered cell volume\n",
    "    \"30130\", #\tMonocyte count\n",
    "    \"30190\", #\tMonocyte percentage\n",
    "    \"30140\", #\tNeutrophill count\n",
    "    \"30200\", #\tNeutrophill percentage\n",
    "    \"30170\", #\tNucleated red blood cell count\n",
    "    \"30230\", #\tNucleated red blood cell percentage\n",
    "    \"30080\", #\tPlatelet count\n",
    "    \"30090\", #\tPlatelet crit\n",
    "    \"30110\", #\tPlatelet distribution width\n",
    "    \"30010\", #\tRed blood cell (erythrocyte) count\n",
    "    \"30070\", #\tRed blood cell (erythrocyte) distribution width\n",
    "    \"30250\", #\tReticulocyte count\n",
    "    \"30240\", #\tReticulocyte percentage\n",
    "    \"30000\", #\tWhite blood cell (leukocyte) count\n",
    "]\n",
    "\n",
    "fields_blood_biochemistry = [\n",
    "    \"30620\",#\tAlanine aminotransferase\n",
    "    \"30600\",#\tAlbumin\n",
    "    \"30610\",#\tAlkaline phosphatase\n",
    "    \"30630\",#\tApolipoprotein A\n",
    "    \"30640\",#\tApolipoprotein B\n",
    "    \"30650\",#\tAspartate aminotransferase\n",
    "    \"30710\",#\tC-reactive protein\n",
    "    \"30680\",#\tCalcium\n",
    "    \"30690\",#\tCholesterol\n",
    "    \"30700\",#\tCreatinine\n",
    "    \"30720\",#\tCystatin C\n",
    "    \"30660\",#\tDirect bilirubin\n",
    "    \"30730\",#\tGamma glutamyltransferase\n",
    "    \"30740\",#\tGlucose\n",
    "    \"30750\",#\tGlycated haemoglobin (HbA1c)\n",
    "    \"30760\",#\tHDL cholesterol\n",
    "    \"30770\",#\tIGF-1\n",
    "    \"30780\",#\tLDL direct\n",
    "    \"30790\",#\tLipoprotein A\n",
    "    \"30800\",#\tOestradiol\n",
    "    \"30810\",#\tPhosphate\n",
    "    \"30820\",#\tRheumatoid factor\n",
    "    \"30830\",#\tSHBG\n",
    "    \"30850\",#\tTestosterone\n",
    "    \"30840\",#\tTotal bilirubin\n",
    "    \"30860\",#\tTotal protein\n",
    "    \"30870\",#\tTriglycerides\n",
    "    \"30880\",#\tUrate\n",
    "    \"30670\",#\tUrea\n",
    "    \"30890\",#\tVitamin D\n",
    "]\n",
    "\n",
    "fields_blood_infectious = [\n",
    "    \"23000\", #\t1gG antigen for Herpes Simplex virus-1\n",
    "    \"23001\", #\t2mgG unique antigen for Herpes Simplex virus-2\n",
    "    \"23049\", #\tAntigen assay QC indicator\n",
    "    \"23048\", #\tAntigen assay date\n",
    "    \"23026\", #\tBK VP1 antigen for Human Polyomavirus BKV\n",
    "    \"23039\", #\tCagA antigen for Helicobacter pylori\n",
    "    \"23043\", #\tCatalase antigen for Helicobacter pylori\n",
    "    \"23018\", #\tCore antigen for Hepatitis C Virus\n",
    "    \"23030\", #\tE6 antigen for Human Papillomavirus type-16\n",
    "    \"23031\", #\tE7 antigen for Human Papillomavirus type-16\n",
    "    \"23006\", #\tEA-D antigen for Epstein-Barr Virus\n",
    "    \"23004\", #\tEBNA-1 antigen for Epstein-Barr Virus\n",
    "    \"23042\", #\tGroEL antigen for Helicobacter pylori\n",
    "    \"23016\", #\tHBc antigen for Hepatitis B Virus\n",
    "    \"23017\", #\tHBe antigen for Hepatitis B Virus\n",
    "    \"23025\", #\tHIV-1 env antigen for Human Immunodeficiency Virus\n",
    "    \"23024\", #\tHIV-1 gag antigen for Human Immunodeficiency Virus\n",
    "    \"23023\", #\tHTLV-1 env antigen for Human T-Lymphotropic Virus 1\n",
    "    \"23022\", #\tHTLV-1 gag antigen for Human T-Lymphotropic Virus 1\n",
    "    \"23010\", #\tIE1A antigen for Human Herpesvirus-6\n",
    "    \"23011\", #\tIE1B antigen for Human Herpesvirus-6\n",
    "    \"23027\", #\tJC VP1 antigen for Human Polyomavirus JCV\n",
    "    \"23015\", #\tK8.1 antigen for Kaposi's Sarcoma-Associated Herpesvirus\n",
    "    \"23029\", #\tL1 antigen for Human Papillomavirus type-16\n",
    "    \"23032\", #\tL1 antigen for Human Papillomavirus type-18\n",
    "    \"23014\", #\tLANA antigen for Kaposi's Sarcoma-Associated Herpesvirus\n",
    "    \"23028\", #\tMC VP1 antigen for Merkel Cell Polyomavirus\n",
    "    \"23019\", #\tNS3 antigen for Hepatitis C Virus\n",
    "    \"23041\", #\tOMP antigen for Helicobacter pylori\n",
    "    \"23037\", #\tPorB antigen for Chlamydia trachomatis\n",
    "    \"23013\", #\tU14 antigen for Human Herpesvirus-7\n",
    "    \"23044\", #\tUreA antigen for Helicobacter pylori\n",
    "    \"23003\", #\tVCA p18 antigen for Epstein-Barr Virus\n",
    "    \"23040\", #\tVacA antigen for Helicobacter pylori\n",
    "    \"23005\", #\tZEBRA antigen for Epstein-Barr Virus\n",
    "    \"23002\", #\tgE / gI antigen for Varicella Zoster Virus\n",
    "    \"23034\", #\tmomp A antigen for Chlamydia trachomatis\n",
    "    \"23033\", #\tmomp D antigen for Chlamydia trachomatis\n",
    "    \"23012\", #\tp101 k antigen for Human Herpesvirus-6\n",
    "    \"23020\", #\tp22 antigen for Toxoplasma gondii\n",
    "    \"23038\", #\tpGP3 antigen for Chlamydia trachomatis\n",
    "    \"23009\", #\tpp 28 antigen for Human Cytomegalovirus\n",
    "    \"23008\", #\tpp 52 antigen for Human Cytomegalovirus\n",
    "    \"23007\", #\tpp150 Nter antigen for Human Cytomegalovirus\n",
    "    \"23021\", #\tsag1 antigen for Toxoplasma gondii\n",
    "    \"23035\", #\ttarp-D F1 antigen for Chlamydia trachomatis\n",
    "    \"23036\", #\ttarp-D F2 antigen for Chlamydia trachomatis\n",
    "]\n",
    "\n",
    "labs = temp = get_data_fields(fields_blood_count+fields_blood_biochemistry+fields_blood_infectious, data, data_field)\n",
    "print(len(temp))\n",
    "display(temp.head())\n",
    "\n",
    "labs.to_feather(os.path.join(path, dataset_path, 'temp_labs.feather'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Family History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-04T12:34:25.838958Z",
     "start_time": "2020-11-04T12:34:07.649920Z"
    }
   },
   "outputs": [],
   "source": [
    "fh_list=[\"Heart disease\", \"Stroke\", \"High blood pressure\",  \"Diabetes\", \"Lung cancer\", \"Severe depression\", \"Parkinson's disease\", \"Alzheimer's disease/dementia\", \"Chronic bronchitis/emphysema\", \"Breast cancer\", \"Bowel cancer\"]\n",
    "with open(os.path.join(path, dataset_path, 'fh_list.yaml'), 'w') as file: yaml.dump(fh_list, file, default_flow_style=False)\n",
    "\n",
    "fields_family_history = [\n",
    "    \"20107\", # Family history \n",
    "    \"20110\" # Family history\n",
    "]\n",
    "\n",
    "raw = get_data_fields(fields_family_history, data, data_field)\n",
    "temp = pd.melt(raw, id_vars=[\"eid\"], value_vars=raw.drop(\"eid\", axis=1).columns.to_list(), var_name = \"field\", value_name=\"family_history\").drop(\"field\", axis=1)\n",
    "temp = temp[temp.family_history.isin(fh_list)].assign(family_history=temp[\"family_history\"].str.lower().replace(\" \", \"_\", regex=True))\n",
    "\n",
    "temp = temp.drop_duplicates().sort_values(\"eid\").reset_index().drop(\"index\", axis=1).assign(n=True)\n",
    "temp = pd.pivot_table(temp, index=\"eid\", columns=\"family_history\", values=\"n\", observed=True).add_prefix('fh_')\n",
    "family_history = temp = data[[\"eid\"]].copy().merge(temp, how=\"left\", on=\"eid\").fillna(False)\n",
    "\n",
    "print(len(temp))\n",
    "temp.head()\n",
    "\n",
    "family_history.to_feather(os.path.join(path, dataset_path, 'temp_family_history.feather'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Medications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-04T12:34:25.842601Z",
     "start_time": "2020-11-04T12:34:25.840607Z"
    }
   },
   "outputs": [],
   "source": [
    "# https://list.essentialmeds.org/?showRemoved=0\n",
    "# essential medicines WHO?!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-04T12:34:32.471831Z",
     "start_time": "2020-11-04T12:34:25.844339Z"
    }
   },
   "outputs": [],
   "source": [
    "atc_mapping = pd.read_csv(f\"{path}/mapping/atc/atc_matched_list.csv\")\n",
    "athena_concepts = pd.read_csv(f\"{data_path}/athena_vocabulary/CONCEPT.csv\", sep=\"\\t\").assign(vocabulary_id = lambda x: x.vocabulary_id.astype(\"string\"), concept_class_id = lambda x: x.concept_class_id.astype(\"string\"))\n",
    "atc_concepts = athena_concepts[athena_concepts.vocabulary_id==\"ATC\"]\n",
    "atc2_concepts = atc_concepts[atc_concepts.concept_class_id==\"ATC 2nd\"].sort_values(\"concept_code\")\n",
    "medication_list = dict(zip([x.lower().replace(\" \", \"_\") for x in atc2_concepts.concept_name.to_list()], [[x] for x in atc2_concepts.concept_code.to_list()]))\n",
    "medication_list_extra = {\n",
    "    \"antihypertensives\": [\"C02\"],\n",
    "    \"statins\": [\"C10A\", \"C10B\"],\n",
    "    \"ass\": [\"B01\"],\n",
    "    \"atypical_antipsychotics\" : [\"N05\"],\n",
    "    \"glucocorticoids\" : [\"H02\"]                        \n",
    "}\n",
    "medication_list.update(medication_list_extra)\n",
    "\n",
    "with open(os.path.join(path, dataset_path, 'medication_list.yaml'), 'w') as file: yaml.dump(medication_list, file, default_flow_style=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-04T12:34:32.481137Z",
     "start_time": "2020-11-04T12:34:32.473698Z"
    }
   },
   "outputs": [],
   "source": [
    "def had_medication_before(data, data_field, medications, atc_mapping):\n",
    "    fields = [\"20003\"]\n",
    "    raw = get_data_fields(fields, data, data_field)\n",
    "    temp = pd.melt(raw, id_vars=[\"eid\"], value_vars=raw.drop(\"eid\", axis=1).columns.to_list(), var_name = \"field\", value_name=\"UKBB_code\").drop(\"field\", axis=1).drop_duplicates()\n",
    "\n",
    "    temp.UKBB_code = temp.UKBB_code.astype(str)\n",
    "    temp = temp[temp.UKBB_code!=\"None\"].copy()\n",
    "    temp = temp[temp.UKBB_code!=\"nan\"].copy()\n",
    "    temp.UKBB_code = temp.UKBB_code.astype(int)\n",
    "\n",
    "    temp_atc = temp.merge(atc_mapping, how=\"left\", on=\"UKBB_code\").sort_values(\"eid\").reset_index(drop=True).dropna(subset=[\"ATC_code\"], axis=0)\n",
    "    temp_atc.ATC_code = temp_atc.ATC_code.astype(\"string\")\n",
    "    temp = data[[\"eid\"]].copy()\n",
    "    for med, med_codes in tqdm(medication_list.items()):\n",
    "        regex_str = \"^\"+\"|^\".join(med_codes)\n",
    "        df = temp_atc[temp_atc.ATC_code.str.contains(regex_str, case=False)][[\"eid\"]]\\\n",
    "            .drop_duplicates(subset=[\"eid\"])\\\n",
    "            .assign(medication=True)\n",
    "        temp[med] = temp.merge(df, how=\"left\", on=\"eid\").fillna(False).medication\n",
    "        \n",
    "    return temp.sort_values(\"eid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-04T12:36:14.425612Z",
     "start_time": "2020-11-04T12:34:32.482863Z"
    }
   },
   "outputs": [],
   "source": [
    "medications = had_medication_before(data, data_field, medication_list, atc_mapping)\n",
    "print(len(medications))\n",
    "medications.head(10)\n",
    "\n",
    "medications.to_feather(os.path.join(path, dataset_path, 'temp_medications.feather'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diagnoses and events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-04T12:37:14.667281Z",
     "start_time": "2020-11-04T12:36:14.427693Z"
    }
   },
   "outputs": [],
   "source": [
    "vocab_dir = f\"{data_path}/athena_vocabulary_covid\"\n",
    "vocab = {\n",
    "    \"concept\": pd.read_csv(f\"{vocab_dir}/CONCEPT.csv\", sep='\\t'),\n",
    "    \"domain\": pd.read_csv(f\"{vocab_dir}/DOMAIN.csv\", sep='\\t'),\n",
    "    \"class\": pd.read_csv(f\"{vocab_dir}/CONCEPT_CLASS.csv\", sep='\\t'),\n",
    "    \"relationship\": pd.read_csv(f\"{vocab_dir}/RELATIONSHIP.csv\", sep='\\t'),\n",
    "    \"drug_strength\": pd.read_csv(f\"{vocab_dir}/DRUG_STRENGTH.csv\", sep='\\t'),\n",
    "    \"vocabulary\": pd.read_csv(f\"{vocab_dir}/VOCABULARY.csv\", sep='\\t'),\n",
    "    \"concept_synonym\": pd.read_csv(f\"{vocab_dir}/CONCEPT_SYNONYM.csv\", sep='\\t'),\n",
    "    \"concept_ancestor\": pd.read_csv(f\"{vocab_dir}/CONCEPT_ANCESTOR.csv\", sep='\\t'),\n",
    "    \"concept_relationship\": pd.read_csv(f\"{vocab_dir}/CONCEPT_RELATIONSHIP.csv\", sep='\\t')                       \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-04T12:37:14.772869Z",
     "start_time": "2020-11-04T12:37:14.669541Z"
    }
   },
   "outputs": [],
   "source": [
    "coding1836 = pd.read_csv(f\"{path}/mapping/codings/coding1836.tsv\", sep=\"\\t\").rename(columns={\"coding\":\"code\"})\n",
    "phecodes = pd.read_csv(f\"{path}/mapping/phecodes/phecode_icd10.csv\")\n",
    "def phenotype_children(phecodes, phenotype_list):\n",
    "    l={}\n",
    "    phecodes = phecodes.dropna(subset=[\"Phenotype\"], axis=0)\n",
    "    for ph, ph_names in phenotype_list.items():\n",
    "        regex = \"|\".join(ph_names)\n",
    "        l[ph] = list(phecodes[phecodes.Phenotype.str.contains(regex, case=False)].ICD10.str.replace(\"\\\\.\", \"\").str.slice(0, 3).unique())\n",
    "    return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-04T12:37:14.851438Z",
     "start_time": "2020-11-04T12:37:14.774599Z"
    }
   },
   "outputs": [],
   "source": [
    "snomed_core = pd.read_csv(f\"{path}/mapping/snomed_core_list.txt\", sep=\"|\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-04T12:37:14.909023Z",
     "start_time": "2020-11-04T12:37:14.853286Z"
    }
   },
   "outputs": [],
   "source": [
    "snomed_core = snomed_core.query(\"SNOMED_CONCEPT_STATUS == 'Current'\").copy()\n",
    "new = snomed_core.SNOMED_FSN.str.split(\"(\", n=1, expand=True)\n",
    "snomed_core[\"snomed_name\"] = new[0].str.rstrip(' ')\n",
    "snomed_core[\"snomed_type\"] = new[1].str.rstrip(')')\n",
    "snomed_core_data = snomed_core#.query(\"(snomed_type=='disorder' | snomed_type=='finding') & USAGE>0.01\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-04T12:37:14.914973Z",
     "start_time": "2020-11-04T12:37:14.910857Z"
    }
   },
   "outputs": [],
   "source": [
    "snomed_names = snomed_core_data.snomed_name.to_list()\n",
    "snomed_names = [str(item).lower().strip().replace(\" \", \"_\").replace(\";\", \"\").replace(\",\", \"\") for item in snomed_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phenotype_list_snomed = dict(zip(snomed_names, snomed_core_data.SNOMED_CID.to_list()))\n",
    "snomed_df = pd.DataFrame.from_dict(phenotype_list_snomed, orient='index').reset_index()\n",
    "snomed_df.columns = [\"diagnosis\", \"concept_code\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab[\"concept_ancestor\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concept_ids = vocab[\"concept\"].query(\"(vocabulary_id == 'SNOMED') | (vocabulary_id == 'ICD10CM')\")\n",
    "concept_ids_icd10 = vocab[\"concept\"].query(\"vocabulary_id == 'ICD10CM'\").concept_id.to_list()\n",
    "vocab_concept_ids = concept_ids.concept_id.to_list()\n",
    "concept_ancestor = vocab[\"concept_ancestor\"][[\"ancestor_concept_id\", \"descendant_concept_id\"]].query(\"ancestor_concept_id == @vocab_concept_ids\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concept_rel = vocab[\"concept_relationship\"][[\"concept_id_1\", \"concept_id_2\", \"relationship_id\"]].query(\"(concept_id_1 == @vocab_concept_ids) & (concept_id_2 == @concept_ids_icd10) & (relationship_id == 'Mapped from')\")\n",
    "concept_mapping = concept_rel.rename(columns={\"concept_id_2\":\"concept_id\"}).merge(concept_ids, on=\"concept_id\").query(\"vocabulary_id == 'ICD10CM'\")[[\"concept_id_1\", \"concept_code\"]].rename(columns={\"concept_id_1\":\"concept_id_desc\",\"concept_code\":\"icd10\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_snomed_concept_id = snomed_df.merge(concept_ids[[\"concept_code\", \"concept_id\"]], on=\"concept_code\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_desc = df_snomed_concept_id.merge(concept_ancestor.rename(columns={\"ancestor_concept_id\":\"concept_id\"})[[\"concept_id\", \"descendant_concept_id\"]], on=\"concept_id\")\n",
    "#df_desc = df_desc[[\"diagnosis\", \"concept_code\", \"concept_id\", \"descendant_concept_id\"]].rename(columns={\"descendant_concept_id\":\"concept_id_desc\"})\n",
    "#df_desc_codes = df_desc.merge(concept_ids[[\"concept_id\", \"concept_code\"]].rename(columns={\"concept_id\":\"concept_id_desc\", \"concept_code\":\"concept_codes_desc\"}), on=\"concept_id_desc\").drop_duplicates().sort_values(\"concept_code\")\n",
    "#df_desc_icd = df_desc_codes.merge(concept_mapping, on=\"concept_id_desc\")#.rename(columns={\"concept_id\":\"concept_id_desc\", \"concept_code\":\"concept_codes_desc\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_icd = df_snomed_concept_id.merge(concept_mapping.rename(columns={\"concept_id_desc\":\"concept_id\"}), on=\"concept_id\")\n",
    "df_mapped = df_icd[[\"diagnosis\", \"concept_code\", \"icd10\"]].drop_duplicates()\n",
    "df_mapped[\"icd10\"] = df_mapped[\"icd10\"].str.replace(\".\", \"\")\n",
    "df_mapped[\"meaning\"] = [e[:3] for e in df_mapped[\"icd10\"].to_list()]\n",
    "icd10_codes = dict(df_mapped[[\"diagnosis\", \"meaning\"]].drop_duplicates().groupby(\"diagnosis\")[\"meaning\"].apply(list).to_dict())#set_index(\"diagnosis\", drop=True).to_dict()[\"meaning\"]#.sort_values(\"diagnosis\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-04T12:39:55.342937Z",
     "start_time": "2020-11-04T12:37:14.916906Z"
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "phenotype_list_snomed = dict(zip(snomed_names, snomed_core_data.SNOMED_CID.to_list()))\n",
    "snomed_ids = vocab[\"concept\"].query(\"vocabulary_id == 'SNOMED'\").concept_id.to_list()\n",
    "icd10_ids = vocab[\"concept\"].query(\"vocabulary_id == 'ICD10CM'\").concept_id.to_list()\n",
    "\n",
    "ph_to_icd10_mapping = {}\n",
    "\n",
    "def map_snomed_to_icd10(ph, snomed_code, concept, concept_ancestor, concept_relationship):\n",
    "    concept_ids = concept.query(\"vocabulary_id == 'SNOMED' & concept_code == @snomed_code\").concept_id.to_list()\n",
    "    snomed_desc_ids = concept_ancestor.query(\"ancestor_concept_id== @concept_ids\").descendant_concept_id.to_list()\n",
    "    ph_desc = concept.query(\"concept_id == @snomed_desc_ids\").query(\"vocabulary_id == 'SNOMED'\")\n",
    "    l_ph_desc_ids = ph_desc.concept_id.to_list()\n",
    "    ph_icd10_ids = list(concept_relationship.query(\"concept_id_1==@l_ph_desc_ids\").query(\"concept_id_2 == @icd10_ids\").concept_id_2.unique())\n",
    "    #ph_icd10_ids = list(concept_relationship.set_index(\"concept_id_1\").query(\"index==@l_ph_desc_ids\").query(\"concept_id_2 == @icd10_ids\").query(\"relationship_id == 'Mapped from'\").concept_id_2.unique()\n",
    "    \n",
    "    #ph_icd10_ids = list(temp.concept_id_2.unique())\n",
    "    df = concept.query(\"concept_id == @ph_icd10_ids & vocabulary_id == 'ICD10CM'\")\n",
    "    icd10_list = list(df[~df.concept_code.str.contains(\"OMOP\", na=False)].concept_code.unique())\n",
    "    icd10_list = sorted(list(set([e[:3] for e in icd10_list])))\n",
    "    #print(f\"{ph}: {icd10_list}\")\n",
    "    return {ph: sorted(list(dict.fromkeys([str(e) for e in icd10_list])))}\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "concept_ids = vocab[\"concept\"].query(\"(vocabulary_id == 'SNOMED') | (vocabulary_id == 'ICD10CM')\")\n",
    "vocab_concept_ids = concept_ids.concept_id.to_list()\n",
    "concept_ancestor = vocab[\"concept_ancestor\"][[\"ancestor_concept_id\", \"descendant_concept_id\"]].query(\"ancestor_concept_id == @vocab_concept_ids\")\n",
    "concept_rel = vocab[\"concept_relationship\"][[\"concept_id_1\", \"concept_id_2\", \"relationship_id\"]].query(\"(concept_id_1 == @vocab_concept_ids) & (concept_id_2 == @vocab_concept_ids) & (relationship_id == 'Mapped from')\")\n",
    "icd10_codes = Parallel(n_jobs=10, require=\"sharedmem\")(delayed(map_snomed_to_icd10)(ph, snomed_code, \n",
    "                                                              concept_ids, concept_ancestor, concept_rel) for ph, snomed_code in tqdm(phenotype_list_snomed.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-04T12:39:55.348009Z",
     "start_time": "2020-11-04T12:39:55.344704Z"
    }
   },
   "outputs": [],
   "source": [
    "l10_snomed = {}\n",
    "for ph in icd10_codes: l10_snomed.update({ph:icd10_codes[ph]})"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-04T12:39:55.468545Z",
     "start_time": "2020-11-04T12:39:55.349545Z"
    }
   },
   "source": [
    "phenotype_list_basic = {\n",
    "    \"myocardial_infarction\": [\"Myocardial infarction\"],\n",
    "    \"stroke\": [\"Cerebrovascular disease\"],\n",
    "    \"diabetes1\" : [\"Type 1 diabetes\"],\n",
    "    \"diabetes2\" : [\"Diabetes mellitus\", \"Type 2 diabetes\"],\n",
    "    \"chronic_kidney_disease\": [\"Chronic kidney disease\", \"chronic renal failure\"],\n",
    "    \"atrial_fibrillation\": [\"Atrial fibrillation\", \"Atrial flutter\", \"paroxysmal tachycardia\"],\n",
    "    \"migraine\": [\"Migraine\"],\n",
    "    \"rheumatoid_arthritis\": [\"Rheumatoid arthritis\"],\n",
    "    \"systemic_lupus_erythematosus\": [\"Systemic lupus erythematosus\"],\n",
    "    \"severe_mental_illness\": [\"Schizophrenia\", \"bipolar\", \"Major depressive disorder\"],\n",
    "    \"erectile_dysfunction\" : [\"Erectile dysfunction\"],  \n",
    "}\n",
    "\n",
    "l10_basic = phenotype_children(phecodes, phenotype_list_basic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l10_basic = {\n",
    "    \"myocardial_infarction\": ['I21', 'I22', 'I23', 'I24', 'I25'],\n",
    "    \"stroke\": ['G45', \"I63\", \"I64\"],\n",
    "    \"diabetes1\" : ['E10'],\n",
    "    \"diabetes2\" : ['E11', 'E12', 'E13', 'E14'],\n",
    "    \"chronic_kidney_disease\": [\"I12\", \"N18\", \"N19\"],\n",
    "    'atrial_fibrillation': ['I47', 'I48'],\n",
    "    'migraine': ['G43', 'G44'],\n",
    "    'rheumatoid_arthritis': ['J99', 'M05', 'M06', 'M08', 'M12', 'M13'],\n",
    "    \"systemic_lupus_erythematosus\": ['M32'],\n",
    "    'severe_mental_illness': ['F20', 'F25', 'F30', 'F31', 'F32', 'F33', 'F44'],\n",
    "    \"erectile_dysfunction\" : ['F52', 'N48'],  \n",
    "    \"liver_disease\":[\"K70\", \"K71\", \"K72\", \"K73\", \"K74\", \"K75\", \"K76\", \"K77\"],\n",
    "    \"dementia\":['F00', 'F01', 'F02', 'F03'],\n",
    "    \"copd\": ['J44']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l10_basic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-04T12:39:55.473556Z",
     "start_time": "2020-11-04T12:39:55.471051Z"
    }
   },
   "outputs": [],
   "source": [
    "l10_all = l10_basic\n",
    "for key, value in l10_snomed.items(): \n",
    "    if key not in l10_basic: l10_all[key] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-04T12:39:55.620916Z",
     "start_time": "2020-11-04T12:39:55.475137Z"
    }
   },
   "outputs": [],
   "source": [
    "l10 = {k: v for k, v in l10_all.items() if len(v)!=0}\n",
    "\n",
    "#phenotype_list = {k: v for k, v in phenotype_list.items() if k in list(l10.keys())}\n",
    "\n",
    "with open(os.path.join(path, dataset_path, 'phenotype_list.yaml'), 'w') as file: yaml.dump(l10, file, default_flow_style=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Self Reported"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-04T12:43:13.288198Z",
     "start_time": "2020-11-04T12:43:13.264683Z"
    }
   },
   "outputs": [],
   "source": [
    "coding609 = pd.read_csv(f\"{path}/mapping/codings/coding609.tsv\", sep=\"\\t\").rename(columns={\"coding\":\"code\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "\n",
    "def datetime_from_dec_year(dec_year):\n",
    "    start = dec_year\n",
    "    year = int(start)\n",
    "    rem = start - year\n",
    "\n",
    "    base = datetime(year, 1, 1)\n",
    "    result = base + timedelta(seconds=(base.replace(year=base.year + 1) - base).total_seconds() * rem)\n",
    "    #result.strftime(\"%Y-%m-%d\")\n",
    "    return result.date()\n",
    "\n",
    "def extract_map_self_reported(data, data_field, code_map):\n",
    "    pbar = tqdm(total=16)\n",
    "    ### codes\n",
    "    fields = [\"20002\"]; pbar.update(1)\n",
    "    raw = get_data_fields_all(fields, data, data_field); pbar.update(1)\n",
    "    col = \"noncancer_illness_code_selfreported_f20002\"; pbar.update(1)\n",
    "    temp = pd.wide_to_long(raw, stubnames=[col], i=\"eid\", j=\"instance_index\", sep=\"_\", suffix=\"\\w+\").reset_index(); pbar.update(1)\n",
    "    codes = temp.rename(columns={col:\"code\"})\\\n",
    "        .assign(code=lambda x: x.code.astype(str))\\\n",
    "        .replace(\"None\", np.nan) \\\n",
    "        .replace(\"nan\", np.nan) \\\n",
    "        .dropna(subset=[\"code\"], axis=0)\\\n",
    "        .assign(code=lambda x: x.code.astype(int)) \\\n",
    "        .merge(code_map, how=\"left\",on=\"code\") \\\n",
    "        .dropna(subset=[\"meaning\"], axis=0)\\\n",
    "        .sort_values([\"eid\", \"instance_index\"]) \\\n",
    "        .reset_index(drop=True); pbar.update(1)\n",
    "    \n",
    "    ### dates\n",
    "    fields = [\"20008\"]; pbar.update(1)\n",
    "    raw = get_data_fields_all(fields, data, data_field); pbar.update(1)\n",
    "    col=\"interpolated_year_when_noncancer_illness_first_diagnosed_f20008\"; pbar.update(1)\n",
    "    temp = pd.wide_to_long(raw, stubnames=[col], i=\"eid\", j=\"instance_index\", sep=\"_\", suffix=\"\\w+\").reset_index(); pbar.update(1)\n",
    "    dates = temp.rename(columns={col:\"date\"})\\\n",
    "        .dropna(subset=[\"date\"], axis=0)\\\n",
    "        .sort_values([\"eid\", \"instance_index\"]) \\\n",
    "        .reset_index(drop=True); pbar.update(1)\n",
    "\n",
    "    dates = dates[dates.date!=-1]; pbar.update(1)\n",
    "    dates = dates[dates.date!=-3]; pbar.update(1)\n",
    "    dates.date = dates.date.apply(datetime_from_dec_year); pbar.update(1)\n",
    "    \n",
    "    test = codes.merge(dates, how=\"left\", on=[\"eid\", \"instance_index\"]).assign(origin=\"self_reported\").copy(); pbar.update(1)\n",
    "    \n",
    "    test[\"instance_index\"] = test[\"instance_index\"].astype(\"string\"); pbar.update(1)\n",
    "    test[['instance','n']] = test.instance_index.str.split(\"_\",expand=True); pbar.update(1)\n",
    "    pbar.close()\n",
    "    \n",
    "    return test[[\"eid\", \"origin\", 'instance','n', \"code\", \"meaning\", \"date\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-04T12:46:00.098893Z",
     "start_time": "2020-11-04T12:43:13.432479Z"
    }
   },
   "outputs": [],
   "source": [
    "codes_self_reported = extract_map_self_reported(data, data_field, coding609)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "codes_gp_records = pd.read_feather(f\"{data_path}/1_decoded/codes_gp_diagnoses_210119.feather\").drop(\"level\", axis=1)\n",
    "codes_hospital_records = pd.read_feather(f\"{data_path}/1_decoded/codes_hes_diagnoses_210120.feather\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine diagnoses and events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-04T12:46:25.115010Z",
     "start_time": "2020-11-04T12:46:08.961388Z"
    }
   },
   "outputs": [],
   "source": [
    "diagnoses_codes = codes_self_reported.append(codes_hospital_records).append(codes_gp_records).sort_values([\"eid\", \"date\"]).dropna(subset=[\"date\"], axis=0).reset_index(drop=True)\n",
    "diagnoses_codes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diagnoses_codes.reset_index(drop=True).assign(eid = lambda x: x.eid.astype(int),\n",
    "                                              origin = lambda x: x.origin.astype(str),\n",
    "                                              instance = lambda x: x.instance.astype(int),\n",
    "                                              n = lambda x: x.n.astype(int),\n",
    "                                              code = lambda x: x.code.astype(str), \n",
    "                                              meaning = lambda x: x.meaning.astype(str))\\\n",
    "    .to_feather(os.path.join(path, dataset_path, 'temp_diagnoses_codes.feather'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diagnoses_codes = pd.read_feather(os.path.join(path, dataset_path, 'temp_diagnoses_codes.feather'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diagnoses_codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-04T12:46:26.189927Z",
     "start_time": "2020-11-04T12:46:25.117069Z"
    }
   },
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed\n",
    "from functools import reduce\n",
    "from numba import jit\n",
    "\n",
    "def had_diagnosis_before_per_ph(df_before, ph, ph_codes, temp):\n",
    "    df_ph = df_before[df_before.meaning.isin(ph_codes)][[\"eid\"]]\\\n",
    "            .drop_duplicates(subset=[\"eid\"])\\\n",
    "            .assign(phenotype=True) \n",
    "    return temp.merge(df_ph, how=\"left\", on=\"eid\").fillna(False).phenotype\n",
    "\n",
    "def had_diagnosis_before(data, diagnoses_codes, phenotypes, time0=time0_col):\n",
    "    diagnoses_codes_time = diagnoses_codes.merge(data[[\"eid\", time0]], how=\"left\", on=\"eid\")\n",
    "    \n",
    "    temp = data[[\"eid\"]].copy()\n",
    "    df_before = diagnoses_codes_time[diagnoses_codes_time.date < diagnoses_codes_time[time0]]\n",
    "                                                                                         \n",
    "    df_phs = Parallel(n_jobs=20, require=\"sharedmem\")(delayed(had_diagnosis_before_per_ph)(df_before, ph, phenotypes[ph], temp) for ph in tqdm(list(phenotypes)))\n",
    "    for ph, df_ph_series in zip(tqdm(list(phenotypes)), df_phs): temp[ph] = df_ph_series#temp.merge(df_ph, how=\"left\", on=\"eid\").fillna(False).phenotype\n",
    "    \n",
    "    return temp.sort_values(\"eid\")  #reduce(lambda left,right: pd.merge(left,right,on=['eid'], how='left'), df_phs)."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "from joblib import Parallel, delayed\n",
    "from functools import reduce\n",
    "from numba import jit\n",
    "\n",
    "def had_diagnosis_before_per_ph(df_before, ph, ph_codes, temp):\n",
    "    regex = \"|\".join(ph_codes)\n",
    "    df_ph = df_before[df_before.meaning.str.contains(regex, case=False)][[\"eid\"]]\\\n",
    "            .drop_duplicates(subset=[\"eid\"])\\\n",
    "            .assign(phenotype=True)   \n",
    "    return temp.merge(df_ph, how=\"left\", on=\"eid\").fillna(False).phenotype\n",
    "\n",
    "def had_diagnosis_before(data, diagnoses_codes, phenotypes, time0=time0_col):\n",
    "    diagnoses_codes_time = diagnoses_codes.merge(data[[\"eid\", time0]], how=\"left\", on=\"eid\")\n",
    "    \n",
    "    temp = data[[\"eid\"]].copy()\n",
    "    df_before = diagnoses_codes_time[diagnoses_codes_time.date < diagnoses_codes_time[time0]]\n",
    "                                                                                         \n",
    "    df_phs = Parallel(n_jobs=30, require=\"sharedmem\")(delayed(had_diagnosis_before_per_ph)(df_before, ph, phenotypes[ph], temp) for ph in tqdm(list(phenotypes)))\n",
    "    for ph, ph_series in zip(tqdm(list(phenotypes)), df_phs): temp[ph] = ph_series\n",
    "    \n",
    "    return temp.sort_values(\"eid\")  #reduce(lambda left,right: pd.merge(left,right,on=['eid'], how='left'), df_phs)."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "phenotypes = l10\n",
    "time0 = time0_col\n",
    "diagnoses_codes_time = diagnoses_codes.merge(data[[\"eid\", time0]], how=\"left\", on=\"eid\")\n",
    "\n",
    "temp = data[[\"eid\"]].copy()\n",
    "df_before = diagnoses_codes_time[diagnoses_codes_time.date < diagnoses_codes_time[time0]]\n",
    "\n",
    "df_phs = Parallel(n_jobs=10, require=\"sharedmem\")(delayed(had_diagnosis_before_per_ph)(df_before, ph, phenotypes[ph], temp) for ph in tqdm(list(phenotypes)[0:100]))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "def had_diagnosis_before(data, diagnoses_codes, phenotypes, time0=time0_col):\n",
    "    diagnoses_codes_time = diagnoses_codes.merge(data[[\"eid\", time0]], how=\"left\", on=\"eid\")\n",
    "    \n",
    "    temp = data[[\"eid\"]].copy()\n",
    "    df_before = diagnoses_codes_time[diagnoses_codes_time.date < diagnoses_codes_time[time0]]\n",
    "    for ph, ph_codes in tqdm(phenotypes.items()):\n",
    "        regex = \"|\".join(ph_codes)\n",
    "        df_ph = df_before[df_before.meaning.str.contains(regex, case=False)][[\"eid\"]]\\\n",
    "            .drop_duplicates(subset=[\"eid\"])\\\n",
    "            .assign(phenotype=True)\n",
    "        temp[ph] = temp.merge(df_ph, how=\"left\", on=\"eid\").fillna(False).phenotype\n",
    "       \n",
    "    return temp.sort_values(\"eid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-04T13:17:52.922023Z",
     "start_time": "2020-11-04T12:46:26.191314Z"
    }
   },
   "outputs": [],
   "source": [
    "diagnoses = had_diagnosis_before(basics, diagnoses_codes, l10, time0=time0_col)\n",
    "print(len(diagnoses))\n",
    "\n",
    "diagnoses.to_feather(os.path.join(path, dataset_path, 'temp_diagnoses.feather'))\n",
    "\n",
    "diagnoses.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diagnoses = pd.read_feather(os.path.join(path, dataset_path, 'temp_diagnoses.feather'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge Everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dfs_dict = {\"basics\": pd.read_feather(os.path.join(path, dataset_path, 'temp_basics.feather')), \n",
    "                 \"questionnaire\": pd.read_feather(os.path.join(path, dataset_path, 'temp_questionnaire.feather')), \n",
    "                 \"measurements\": pd.read_feather(os.path.join(path, dataset_path, 'temp_measurements.feather')), \n",
    "                 \"labs\": pd.read_feather(os.path.join(path, dataset_path, 'temp_labs.feather')), \n",
    "                 \"family_history\": pd.read_feather(os.path.join(path, dataset_path, 'temp_family_history.feather')), \n",
    "                 \"diagnoses\": pd.read_feather(os.path.join(path, dataset_path, 'temp_diagnoses.feather')),\n",
    "                 \"medications\": pd.read_feather(os.path.join(path, dataset_path, 'temp_medications.feather'))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cols_clean(df):\n",
    "    df.columns = df.columns.str.replace(r'_0_0$', '').str.replace(r'_f[0-9]+$', '').str.replace(\"_automated_reading\", '')\n",
    "    return df.columns\n",
    "\n",
    "def clean_df(df):\n",
    "    df.columns = get_cols_clean(df)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from functools import reduce\n",
    "\n",
    "data_baseline = reduce(lambda x, y: pd.merge(x, y, on = 'eid'), list(data_dfs_dict.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_baseline = clean_df(data_baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in [col for col in list(data_baseline.columns) if (\"_event\" in col) & (\"_time\" not in col)]:\n",
    "    data_baseline[col] = data_baseline[col].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "covariates = [col for col in list(data_baseline.columns) if not \"_event\" in col]\n",
    "targets = [col for col in list(data_baseline.columns) if \"_event\" in col]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exporting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_cols = {}\n",
    "for topic, df in data_dfs_dict.items(): \n",
    "    data_cols[\"eid\"] = [\"admin\"]\n",
    "    data_cols[topic]=list(get_cols_clean(df))[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_cols_single = {}\n",
    "for topic, columns in data_cols.items():\n",
    "    for col in columns:\n",
    "        data_cols_single[col] = topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtypes = {\"int32\":\"int\", \"int64\":\"int\", \"float64\":\"float\", \"category\":\"category\", \"object\":\"category\", \"bool\":\"bool\"}\n",
    "desc_dict = {\"id\": [*range(1, len(data_baseline.columns.to_list())+1)] , \n",
    "             \"covariate\": data_baseline.columns.to_list(), \n",
    "             \"dtype\":[dtypes[str(col)] for col in data_baseline.dtypes.to_list()], \n",
    "             \"isTarget\":[True if col in targets else False for col in data_baseline.columns.to_list()],\n",
    "            \"based_on\":[topic for col, topic in data_cols_single.items()],\n",
    "            \"aggr_fn\": [np.nan for col in data_baseline.columns.to_list()]}\n",
    "data_baseline_description = pd.DataFrame.from_dict(desc_dict)\n",
    "data_baseline_description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_dict = {}\n",
    "for group in data_baseline_description.based_on.unique(): feature_dict[group] = data_baseline_description.query(\"based_on==@group\").covariate.to_list()\n",
    "with open(os.path.join(path, dataset_path, 'feature_list.yaml'), 'w') as file: yaml.dump(feature_dict, file, default_flow_style=False, allow_unicode=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_baseline.to_feather(os.path.join(path, dataset_path, 'baseline_covariates.feather'))\n",
    "data_baseline_description.to_feather(os.path.join(path, dataset_path, 'baseline_covariates_description.feather'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:miniconda3-pl1.x]",
   "language": "python",
   "name": "conda-env-miniconda3-pl1.x-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  },
  "toc-autonumbering": true,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
