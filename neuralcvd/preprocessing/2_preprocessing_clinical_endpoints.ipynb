{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-04T12:31:49.436340Z",
     "start_time": "2020-11-04T12:31:48.732042Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import yaml\n",
    "from tqdm.notebook import trange, tqdm\n",
    "dataset_name = \"210212_cvd_gp\"\n",
    "path = \"/data/analysis/ag-reils/steinfej/code/umbrella/pre/ukbb\"\n",
    "data_path = \"/data/analysis/ag-reils/ag-reils-shared/cardioRS/data\"\n",
    "dataset_path = f\"{data_path}/2_datasets_pre/{dataset_name}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-04T12:31:49.895222Z",
     "start_time": "2020-11-04T12:31:49.891332Z"
    }
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "Path(dataset_path).mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-04T12:33:14.171198Z",
     "start_time": "2020-11-04T12:31:50.204540Z"
    }
   },
   "outputs": [],
   "source": [
    "data = pd.read_feather(f\"{data_path}/1_decoded/ukb_data.feather\")\n",
    "data_field = pd.read_feather(f\"{data_path}/1_decoded/ukb_data_field.feather\")\n",
    "data_columns = data.columns.to_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mappings + Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-04T12:34:05.867152Z",
     "start_time": "2020-11-04T12:33:16.878773Z"
    }
   },
   "outputs": [],
   "source": [
    "# Drop obviouse missing data\n",
    "print(len(data))\n",
    "data = data.dropna(subset=[\"sex_f31_0_0\"], axis=0)\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Starting information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-04T12:34:05.872216Z",
     "start_time": "2020-11-04T12:34:05.869505Z"
    }
   },
   "outputs": [],
   "source": [
    "#time0_col=\"birth_date\"\n",
    "time0_col=\"date_of_attending_assessment_centre_f53_0_0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline covariates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-04T12:34:05.889725Z",
     "start_time": "2020-11-04T12:34:05.874587Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_fields(fields, data, data_field):\n",
    "    f = data_field[data_field[\"field.showcase\"].isin(fields) & data_field[\"field.tab\"].str.contains(\"f\\\\.\\\\d+\\\\.0\\\\.\\\\d\")].copy()\n",
    "    f[\"field\"] = pd.Categorical(f[\"field.showcase\"], categories=fields, ordered=True)\n",
    "    f = f.sort_values(\"field\").reset_index().drop(\"field\", axis=1)\n",
    "    return f\n",
    "\n",
    "def get_fields_all(fields, data, data_field):\n",
    "    f = data_field[data_field[\"field.showcase\"].isin(fields)].copy()\n",
    "    f[\"field\"] = pd.Categorical(f[\"field.showcase\"], categories=fields, ordered=True)\n",
    "    f = f.sort_values(\"field\").reset_index().drop(\"field\", axis=1)\n",
    "    return f\n",
    "\n",
    "def get_data_fields(fields, data, data_field):\n",
    "    f = get_fields(fields, data, data_field)\n",
    "    return data[[\"eid\"]+f[\"col.name\"].to_list()].copy()\n",
    "\n",
    "def get_data_fields_all(fields, data, data_field):\n",
    "    f = get_fields_all(fields, data, data_field)\n",
    "    return data[[\"eid\"]+f[\"col.name\"].to_list()].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diagnoses and events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-04T12:37:14.667281Z",
     "start_time": "2020-11-04T12:36:14.427693Z"
    }
   },
   "outputs": [],
   "source": [
    "vocab_dir = f\"{data_path}/athena_vocabulary_covid\"\n",
    "vocab = {\n",
    "    \"concept\": pd.read_csv(f\"{vocab_dir}/CONCEPT.csv\", sep='\\t'),\n",
    "    \"domain\": pd.read_csv(f\"{vocab_dir}/DOMAIN.csv\", sep='\\t'),\n",
    "    \"class\": pd.read_csv(f\"{vocab_dir}/CONCEPT_CLASS.csv\", sep='\\t'),\n",
    "    \"relationship\": pd.read_csv(f\"{vocab_dir}/RELATIONSHIP.csv\", sep='\\t'),\n",
    "    \"drug_strength\": pd.read_csv(f\"{vocab_dir}/DRUG_STRENGTH.csv\", sep='\\t'),\n",
    "    \"vocabulary\": pd.read_csv(f\"{vocab_dir}/VOCABULARY.csv\", sep='\\t'),\n",
    "    \"concept_synonym\": pd.read_csv(f\"{vocab_dir}/CONCEPT_SYNONYM.csv\", sep='\\t'),\n",
    "    \"concept_ancestor\": pd.read_csv(f\"{vocab_dir}/CONCEPT_ANCESTOR.csv\", sep='\\t'),\n",
    "    \"concept_relationship\": pd.read_csv(f\"{vocab_dir}/CONCEPT_RELATIONSHIP.csv\", sep='\\t')                       \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-04T12:37:14.772869Z",
     "start_time": "2020-11-04T12:37:14.669541Z"
    }
   },
   "outputs": [],
   "source": [
    "coding1836 = pd.read_csv(f\"{path}/mapping/codings/coding1836.tsv\", sep=\"\\t\").rename(columns={\"coding\":\"code\"})\n",
    "phecodes = pd.read_csv(f\"{path}/mapping/phecodes/phecode_icd10.csv\")\n",
    "def phenotype_children(phecodes, phenotype_list):\n",
    "    l={}\n",
    "    phecodes = phecodes.dropna(subset=[\"Phenotype\"], axis=0)\n",
    "    for ph, ph_names in phenotype_list.items():\n",
    "        regex = \"|\".join(ph_names)\n",
    "        l[ph] = list(phecodes[phecodes.Phenotype.str.contains(regex, case=False)].ICD10.str.replace(\"\\\\.\", \"\").str.slice(0, 3).unique())\n",
    "    return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diagnoses_codes = pd.read_feather(os.path.join(path, dataset_path, 'temp_diagnoses_codes.feather')).drop(\"level\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "death_codes = pd.read_feather(f\"{data_path}/1_decoded/codes_death_records_210115.feather\").query(\"level==1\").drop(\"level\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_codes = pd.concat([diagnoses_codes, death_codes[diagnoses_codes.columns]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Endpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-04T12:39:55.628580Z",
     "start_time": "2020-11-04T12:33:33.036Z"
    }
   },
   "outputs": [],
   "source": [
    "### define in snomed and get icd codes from there"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Hospital admissions"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-04T12:39:55.629459Z",
     "start_time": "2020-11-04T12:33:34.764Z"
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "endpoint_list = {\n",
    "    \"myocardial_infarction\": [\"Myocardial infarction\"],\n",
    "    \"stroke\": [\"Cerebrovascular disease\"],\n",
    "    \"cancer_breast\" : [\"Breast Cancer\"],\n",
    "    \"diabetes\" : [\"Diabetes\"],\n",
    "    \"atrial_fibrillation\": [\"Atrial fibrillation\", \"Atrial flutter\", \"paroxysmal tachycardia\"],\n",
    "    \"copd\": [\"COPD\"],\n",
    "    \"dementia\":[\"dementia\"]\n",
    "}\n",
    "\n",
    "endpoint_list = phenotype_children(phecodes, endpoint_list)\n",
    "\n",
    "\n",
    "\n",
    "endpoint_list[\"cancer_breast\"] = [\"C50\"]\n",
    "endpoint_list[\"copd\"] = [\"J44\"]\n",
    "endpoint_list[\"diabetes\"] = [\"E10\", \"E11\", \"E12\", \"E13\", \"E14\"]\n",
    "endpoint_list[\"atrial_fibrillation\"] = [\"I47\", \"I48\"]\n",
    "\n",
    "\n",
    "with open(os.path.join(path, dataset_path, 'endpoint_list.yaml'), 'w') as file: yaml.dump(endpoint_list, file, default_flow_style=False)\n",
    "endpoint_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_list = {\n",
    "    \"myocardial_infarction\": ['I21', 'I22', 'I23', 'I24', 'I25'],\n",
    "    \"stroke\": ['G45', \"I63\", \"I64\"],\n",
    "    \"diabetes\" : ['E10', 'E11', 'E12', 'E13', 'E14'],\n",
    "    \"diabetes1\" : ['E10'],\n",
    "    \"diabetes2\" : ['E11', 'E12', 'E13', 'E14'],\n",
    "    \"atrial_fibrillation\": ['I47', 'I48'],\n",
    "    'migraine': ['G43', 'G44'],\n",
    "    'rheumatoid_arthritis': ['J99', 'M05', 'M06', 'M08', 'M12', 'M13'],\n",
    "    \"systemic_lupus_erythematosus\": ['M32'],\n",
    "    'severe_mental_illness': ['F20', 'F25', 'F30', 'F31', 'F32', 'F33', 'F44'],\n",
    "    \"erectile_dysfunction\" : ['F52', 'N48'],  \n",
    "    \"chronic_kidney_disease\": [\"I12\", \"N18\", \"N19\"],\n",
    "    \"liver_disease\":[\"K70\", \"K71\", \"K72\", \"K73\", \"K74\", \"K75\", \"K76\", \"K77\"],\n",
    "    \"dementia\":['F00', 'F01', 'F02', 'F03'],\n",
    "    \"copd\": ['J44']}\n",
    "\n",
    "with open(os.path.join(path, dataset_path, 'endpoint_list.yaml'), 'w') as file: yaml.dump(endpoint_list, file, default_flow_style=False)\n",
    "endpoint_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dateutil.relativedelta import relativedelta\n",
    "import datetime\n",
    "\n",
    "def extract_endpoints_tte(data, diagnoses_codes, endpoint_list, time0_col, level=None):\n",
    "    if level is not None: diagnoses_codes = diagnoses_codes.query(\"level==@level\")\n",
    "    diagnoses_codes_time0 = diagnoses_codes.merge(data[[\"eid\", time0_col]], how=\"left\", on=\"eid\")\n",
    "    \n",
    "    #cens_time_right = max(diagnoses_codes.sort_values('date').groupby('origin').tail(1).date.to_list())\n",
    "    cens_time_right = datetime.date(2020, 9, 30)\n",
    "    print(f\"t_0: {time0_col}\")\n",
    "    print(f\"t_cens: {cens_time_right}\")\n",
    "    \n",
    "    df_interval = diagnoses_codes_time0[(diagnoses_codes_time0.date > diagnoses_codes_time0[time0_col]) & \n",
    "                                        (diagnoses_codes_time0.date < cens_time_right)]\n",
    "    \n",
    "    temp = data[[\"eid\", time0_col]].copy()\n",
    "    for ph, ph_codes in tqdm(endpoint_list.items()):\n",
    "        regex = \"|\".join(ph_codes)\n",
    "        ph_df = df_interval[df_interval.meaning.str.contains(regex, case=False)] \\\n",
    "            .sort_values('date').groupby('eid').head(1).assign(phenotype=1, date=lambda x: x.date)\n",
    "        temp_ph = temp.merge(ph_df, how=\"left\", on=\"eid\").fillna(0)\n",
    "        temp[ph+\"_event\"], temp[ph+\"_event_date\"] = temp_ph.phenotype, temp_ph.date\n",
    "        \n",
    "        fill_date = {ph+\"_event_date\" : lambda x: [cens_time_right if event==0 else event_date for event, event_date in zip(x[ph+\"_event\"], x[ph+\"_event_date\"])]}\n",
    "        calc_tte = {ph+\"_event_time\" : lambda x: [(event_date-time0).days/365.25  for time0, event_date in zip(x[time0_col], x[ph+\"_event_date\"])]}\n",
    "        \n",
    "        temp = temp.assign(**fill_date).assign(**calc_tte).drop([ph+\"_event_date\"], axis=1)\n",
    "        \n",
    "    temp = temp.drop([time0_col], axis=1)     \n",
    "    \n",
    "    return temp.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basics = pd.read_feather(os.path.join(path, dataset_path, 'temp_basics.feather'))\n",
    "endpoints_diagnoses = extract_endpoints_tte(basics, endpoint_codes, endpoint_list, time0_col)\n",
    "print(len(endpoints_diagnoses))\n",
    "endpoints_diagnoses.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Death registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "death_list = {\n",
    "    \"death_allcause\":[],\n",
    "    \"death_cvd\":['I{:02}'.format(ID+1) for ID in range(0, 98)],\n",
    "}\n",
    "\n",
    "with open(os.path.join(path, dataset_path, 'death_list.yaml'), 'w') as file: yaml.dump(death_list, file, default_flow_style=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoints_death = extract_endpoints_tte(basics, death_codes, death_list, time0_col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SCORES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_list = {\n",
    "    \"SCORE\":['I{:02}'.format(ID) for ID in [10, 11, 12, 13, 14, 15, 20, 21, 22, 23, 24, 25, 44, 45, 46, 47, 48, 49, 50, 51, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73]],\n",
    "    \"ASCVD\":['I{:02}'.format(ID) for ID in [20, 21, 22, 23, 24, 25, 63]],\n",
    "    \"QRISK3\":[\"G45\", \"I20\", \"I21\", \"I22\", \"I23\", \"I24\", \"I25\", \"I63\", \"I64\"],\n",
    "    \"MACE\":[\"G45\", \"I21\", \"I22\", \"I23\", \"I24\", \"I25\", \"I63\", \"I64\"],    \n",
    "}\n",
    "with open(os.path.join(path, dataset_path, 'scores_list.yaml'), 'w') as file: yaml.dump(scores_list, file, default_flow_style=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "death_scores =  extract_endpoints_tte(basics, death_codes, scores_list, time0_col=time0_col)\n",
    "endpoint_scores = extract_endpoints_tte(basics, endpoint_codes, scores_list, time0_col=time0_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoints_scores_all = death_scores[[\"eid\", \"SCORE_event\", \"SCORE_event_time\"]].merge(endpoint_scores[[\"eid\", \"ASCVD_event\", \"ASCVD_event_time\", \"QRISK3_event\", \"QRISK3_event_time\", \"MACE_event\", \"MACE_event_time\"]], on=\"eid\")\n",
    "endpoints_scores_all.to_feather(os.path.join(path, dataset_path, 'temp_endpoints_scores_all.feather'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ESC SCORE (Conroy 2003)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = \"SCORE\"\n",
    "print(len(endpoints_scores_all.query(score+\"_event==1\")))\n",
    "endpoints_scores_all.query(score+\"_event==1\").head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ASCVD (Goff 2014)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = \"ASCVD\"\n",
    "print(len(endpoints_scores_all.query(score+\"_event==1\")))\n",
    "endpoints_scores_all.query(score+\"_event==1\").head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UK QRISK3 (2017)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = \"QRISK3\"\n",
    "print(len(endpoints_scores_all.query(score+\"_event==1\")))\n",
    "endpoints_scores_all.query(score+\"_event==1\").head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MACE (2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = \"MACE\"\n",
    "print(len(endpoints_scores_all.query(score+\"_event==1\")))\n",
    "endpoints_scores_all.query(score+\"_event==1\").head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge Everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dfs_dict = {\"endpoints_diagnoses\":endpoints_diagnoses, \n",
    "                 \"endpoints_death\":endpoints_death, \n",
    "                 \"endpoints_scores_all\":endpoints_scores_all}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cols_clean(df):\n",
    "    df.columns = df.columns.str.replace(r'_0_0$', '').str.replace(r'_f[0-9]+$', '').str.replace(\"_automated_reading\", '')\n",
    "    return df.columns\n",
    "\n",
    "def clean_df(df):\n",
    "    df.columns = get_cols_clean(df)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from functools import reduce\n",
    "\n",
    "data_baseline = reduce(lambda x, y: pd.merge(x, y, on = 'eid'), list(data_dfs_dict.values()))\n",
    "endpoint_columns = [c[:-11] for c in data_baseline.columns.tolist() if \"_event_time\" in c]\n",
    "print(endpoint_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Competing Events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# endpoint < death -> 1\n",
    "# death < endpoint -> 2\n",
    "# time min(endpoint_time, death_time) -> time\n",
    "def event_calc(endpoint, endpoint_time, death, death_time):\n",
    "    endpoint = int(endpoint)\n",
    "    death = int(death)\n",
    "    if (endpoint==0) and (death==0): \n",
    "        return 0.0\n",
    "    if (endpoint==1) and (death==0): \n",
    "        return 1.0\n",
    "    elif (endpoint==0) and (death==1): \n",
    "        return 2.0\n",
    "    elif (endpoint==1) and (death==1) and (endpoint_time<=death_time):\n",
    "        return float(1)\n",
    "    elif (endpoint==1) and (death==1) and (death_time<endpoint_time):\n",
    "        return float(2)\n",
    "    else: return np.nan\n",
    "\n",
    "for c in tqdm(endpoint_columns): \n",
    "    if c!=\"death_allcause\":\n",
    "        data_baseline[f\"{c}_comp_event\"] = [event_calc(endpoint, endpoint_time, death, death_time) for endpoint, endpoint_time, death, death_time \n",
    "                                            in zip(data_baseline[f\"{c}_event\"], data_baseline[f\"{c}_event_time\"], data_baseline[\"death_allcause_event\"], data_baseline[\"death_allcause_event_time\"])]\n",
    "        \n",
    "        data_baseline[f\"{c}_comp_event_time\"] = [min(endpoint_time, death_time)\n",
    "                                                 for endpoint, endpoint_time, death, death_time\n",
    "                                                 in zip(data_baseline[f\"{c}_event\"], data_baseline[f\"{c}_event_time\"],\n",
    "                                                        data_baseline[\"death_allcause_event\"], data_baseline[\"death_allcause_event_time\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_baseline = clean_df(data_baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in [col for col in list(data_baseline.columns) if (\"_event\" in col) & (\"_time\" not in col)]:\n",
    "    data_baseline[col] = data_baseline[col].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "covariates = [col for col in list(data_baseline.columns) if not \"_event\" in col]\n",
    "targets = [col for col in list(data_baseline.columns) if \"_event\" in col]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exporting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_cols = {}\n",
    "for topic, df in data_dfs_dict.items(): \n",
    "    data_cols[\"eid\"] = [\"admin\"]\n",
    "    data_cols[topic]=list(get_cols_clean(df))[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_cols_single = {}\n",
    "for topic, columns in data_cols.items():\n",
    "    for col in columns:\n",
    "        data_cols_single[col] = topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in [c for c in data_baseline.columns.tolist() if \"comp\" in c]:\n",
    "    data_cols_single.update({c:\"endpoints_competing\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtypes = {\"int32\":\"int\", \"int64\":\"int\", \"float64\":\"float\", \"category\":\"category\", \"object\":\"category\", \"bool\":\"bool\"}\n",
    "desc_dict = {\"id\": [*range(1, len(data_baseline.columns.to_list())+1)] , \n",
    "             \"covariate\": data_baseline.columns.to_list(), \n",
    "             \"dtype\":[dtypes[str(col)] for col in data_baseline.dtypes.to_list()], \n",
    "             \"isTarget\":[True if col in targets else False for col in data_baseline.columns.to_list()],\n",
    "            \"based_on\":[topic for col, topic in data_cols_single.items()],\n",
    "            \"aggr_fn\": [np.nan for col in data_baseline.columns.to_list()]}\n",
    "data_baseline_description = pd.DataFrame.from_dict(desc_dict)\n",
    "data_baseline_description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_baseline.to_feather(os.path.join(path, dataset_path, 'baseline_endpoints.feather'))\n",
    "data_baseline_description.to_feather(os.path.join(path, dataset_path, 'baseline_endpoints_description.feather'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:miniconda3-pl1.x]",
   "language": "python",
   "name": "conda-env-miniconda3-pl1.x-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  },
  "toc-autonumbering": true,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
